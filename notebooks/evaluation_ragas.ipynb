{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from mtg.bot import mtg_chain\n",
    "\n",
    "with open(\"configs/config.yaml\", \"r\") as infile:\n",
    "    config = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "\n",
    "llm = mtg_chain.create_llm(model_name=\"gpt-3.5-turbo-0125\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expert_knowledge', 'level_0', 'level_1', 'level_2', 'stackexchange']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = Path(\"../data/etl/\")\n",
    "\n",
    "evaluation_dataset = {}\n",
    "for file in (DATA_PATH / \"processed/evaluation\").iterdir():\n",
    "    with file.open(\"r\", encoding=\"utf-8\") as infile:\n",
    "        dataset = json.load(infile)\n",
    "    for key, value in dataset.items(): \n",
    "        evaluation_dataset[key] = value\n",
    "\n",
    "list(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|██████▌   | 13/20 [00:08<00:03,  1.84it/s]WARNING:ragas.metrics._context_recall:Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating: 100%|██████████| 20/20 [00:37<00:00,  1.87s/it]\n",
      "Evaluating:  75%|███████▌  | 15/20 [01:04<00:17,  3.56s/it]WARNING:ragas.metrics._answer_relevance:Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 20/20 [01:40<00:00,  5.02s/it]\n",
      "Evaluating:   5%|▌         | 1/20 [00:02<00:40,  2.12s/it]WARNING:ragas.metrics._context_recall:Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating:  60%|██████    | 12/20 [00:08<00:04,  1.62it/s]WARNING:ragas.metrics._answer_relevance:Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 20/20 [01:05<00:00,  3.28s/it]\n",
      "Evaluating:  65%|██████▌   | 13/20 [00:25<00:22,  3.26s/it]WARNING:ragas.metrics._answer_relevance:Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating:  95%|█████████▌| 19/20 [01:03<00:06,  6.72s/it]WARNING:ragas.metrics._answer_relevance:Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 20/20 [01:16<00:00,  3.83s/it]\n",
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]WARNING:ragas.metrics._context_recall:Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating:  10%|█         | 2/20 [00:02<00:15,  1.16it/s]WARNING:ragas.metrics._context_recall:Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating: 100%|██████████| 20/20 [01:51<00:00,  5.58s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for key in evaluation_dataset:\n",
    "    for d in evaluation_dataset[key]: \n",
    "        if \"context\" in d:\n",
    "            d[\"contexts\"] = d[\"context\"]\n",
    "        \n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(evaluation_dataset[key]))\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_recall,\n",
    "            context_precision,\n",
    "        ],\n",
    "        llm=llm,\n",
    "    )\n",
    "    results[key] = result\n",
    "    results[key][\"number_of_questions\"] = len(evaluation_dataset[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "-  <b>Faithfullness / Groundedness</b>  \n",
    "    This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "    The generated answer is regarded as faithful if all the claims that are made in the answer can be inferred from the given context. To calculate this a set of claims from the generated answer is first identified. Then each one of these claims are cross checked with given context to determine if it can be inferred from given context or not.  \n",
    "\n",
    "-  <b>Answer Relevancy</b>  \n",
    "    The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the question, the context and the answer.\n",
    "\n",
    "-  <b>Context Recall</b>  \n",
    "    Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "    To estimate context recall from the ground truth answer, each sentence in the ground truth answer is analyzed to determine whether it can be attributed to the retrieved context or not. In an ideal scenario, all sentences in the ground truth answer should be attributable to the retrieved context.  \n",
    "    \n",
    "-  <b>Context Precision</b>   \n",
    "    Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>number_of_questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>expert_knowledge</th>\n",
       "      <td>0.845598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.883579</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_0</th>\n",
       "      <td>0.673718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.754995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stackexchange</th>\n",
       "      <td>0.852136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928552</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_1</th>\n",
       "      <td>0.936661</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.889048</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_2</th>\n",
       "      <td>0.970543</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.947956</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  answer_relevancy  faithfulness  context_recall  \\\n",
       "expert_knowledge          0.845598           1.0             1.0   \n",
       "level_0                   0.673718           1.0             1.0   \n",
       "stackexchange             0.852136           1.0             1.0   \n",
       "level_1                   0.936661           1.0             0.9   \n",
       "level_2                   0.970543           1.0             0.8   \n",
       "\n",
       "                  context_precision  number_of_questions  \n",
       "expert_knowledge           0.883579                    5  \n",
       "level_0                    0.754995                    5  \n",
       "stackexchange              0.928552                    5  \n",
       "level_1                    0.889048                    5  \n",
       "level_2                    0.947956                    5  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "results_df = results_df.sort_values([\"faithfulness\", \"context_recall\"], ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(DATA_PATH / \"evaluation_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
